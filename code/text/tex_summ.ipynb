{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, List\n",
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "# from IPython.display import Image\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import os\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "from filelock import FileLock\n",
    "import time\n",
    "import unicodedata\n",
    "import pickle\n",
    "import json\n",
    "from summarizer import Summarizer,TransformerSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    # Remove all punctuation and whitespace\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Zs' and unicodedata.category(char) != 'P')\n",
    "    \n",
    "    # Calculate the percentage of non-English characters\n",
    "    english_chars = sum(1 for char in text if 'a' <= char.lower() <= 'z')\n",
    "    non_english_chars = len(text) - english_chars\n",
    "    non_english_percentage = (non_english_chars / len(text)) * 100\n",
    "    \n",
    "    # Return True if the text is at least 90% English characters\n",
    "    return non_english_percentage <= 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/wine/code/feature_map/item2idx.json','r') as f: item2idx = json.load(f)\n",
    "basic_info = pd.read_csv('/opt/ml/wine/data/wine_df.csv')\n",
    "basic_info['wine_id'] = basic_info['url'].map(item2idx)\n",
    "basic_info = basic_info[basic_info['wine_id'].isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('/opt/ml/wine/data/review_df_cleaned.csv',encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_info['wine_id'] = basic_info['wine_id'].astype('int').astype('category')\n",
    "review_df['wine_id'] = review_df['wine_id'].astype('int').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151746/1151746 [46:18<00:00, 414.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "review_df['eng_or_na'] = review_df['text'].progress_apply(is_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_review= review_df[review_df['eng_or_na']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_review = pd.merge(eng_review, basic_info.loc[:,['wine_id','wine_style']], on = 'wine_id',how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_review =eng_review[eng_review['wine_style'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reviews = eng_review.groupby('wine_style').agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_styles = basic_info['wine_style'].value_counts().index[:150]\n",
    "top_reviews = merged_reviews.loc[merged_reviews.index.isin(top_styles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine_style</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alsace Pinot Gris</th>\n",
       "      <td>Copper, salted caramel, green apple .Full body...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alsace Pinot Noir</th>\n",
       "      <td>Cerises fruits rouges .Nice light red taste.Qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alsace Riesling</th>\n",
       "      <td>Ingen nse. Udvandede abrikos.Vin trs bien fait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Argentinian Cabernet Sauvignon</th>\n",
       "      <td>Bom vinho.Suave frutal.No gostei.No gostei.mui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Argentinian Chardonnay</th>\n",
       "      <td>Clean crisp, like a frosty winters morning. Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish Toro Red</th>\n",
       "      <td>Complex, burnt wood, bold.Good wine, do over.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Upper Loire White</th>\n",
       "      <td>Some floral tones on the nose. Mineral flint, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington State Cabernet Sauvignon</th>\n",
       "      <td>Smooth Cab with dinner or by itself. I didnt r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington State Merlot</th>\n",
       "      <td>A nice Merlot smooth, a little jammy upfront, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington State Red Blend</th>\n",
       "      <td>One of the.Smooth round fruit.As advertised ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  text\n",
       "wine_style                                                                            \n",
       "Alsace Pinot Gris                    Copper, salted caramel, green apple .Full body...\n",
       "Alsace Pinot Noir                    Cerises fruits rouges .Nice light red taste.Qu...\n",
       "Alsace Riesling                      Ingen nse. Udvandede abrikos.Vin trs bien fait...\n",
       "Argentinian Cabernet Sauvignon       Bom vinho.Suave frutal.No gostei.No gostei.mui...\n",
       "Argentinian Chardonnay               Clean crisp, like a frosty winters morning. Cl...\n",
       "...                                                                                ...\n",
       "Spanish Toro Red                     Complex, burnt wood, bold.Good wine, do over.S...\n",
       "Upper Loire White                    Some floral tones on the nose. Mineral flint, ...\n",
       "Washington State Cabernet Sauvignon  Smooth Cab with dinner or by itself. I didnt r...\n",
       "Washington State Merlot              A nice Merlot smooth, a little jammy upfront, ...\n",
       "Washington State Red Blend           One of the.Smooth round fruit.As advertised ex...\n",
       "\n",
       "[98 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [28:38, 728.00s/it]/opt/conda/lib/python3.8/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (43) found smaller than n_clusters (717). Possibly due to duplicate points in X.\n",
      "  model = self._get_model(k).fit(self.features)\n",
      "4it [48:59, 922.81s/it]/opt/conda/lib/python3.8/site-packages/summarizer/cluster_features.py:149: ConvergenceWarning: Number of distinct clusters (614) found smaller than n_clusters (1129). Possibly due to duplicate points in X.\n",
      "  model = self._get_model(k).fit(self.features)\n",
      "5it [2:01:00, 2148.11s/it]"
     ]
    }
   ],
   "source": [
    "style_summary = {}\n",
    "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
    "for style, text in tqdm(zip(top_reviews.index, top_reviews['text'])):\n",
    "    text = text[:1000000]\n",
    "    style_summary[style] = ''.join(GPT2_model(text, min_length=30, max_length=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/wine/data/wine_style_summary.json','w') as f: json.dump(style_summary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_english_and_digits(text):\n",
    "    # Remove any characters that are not English alphabets, digits, periods, or commas at the end of sentences\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reviews['text'] = merged_reviews['text'].apply(keep_english_and_digits)\n",
    "merged_reviews = merged_reviews.sort_values(by = 'wine_id')\n",
    "merged_reviews.to_csv('/opt/ml/wine/data/merged_review.csv',encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reviews = pd.read_csv('/opt/ml/wine/data/merged_review.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text(data):\n",
    "    return '\\n'.join(data)\n",
    "\n",
    "def parallel(func, input,  num_cpu):\n",
    "\n",
    "    chunks = np.array_split(input, num_cpu)\n",
    "\n",
    "    print('Parallelizing with ' +str(num_cpu)+'cores')\n",
    "    with Parallel(n_jobs = num_cpu, backend=\"multiprocessing\") as parallel:\n",
    "        results = parallel(delayed(func)(chunks[i]) for i in range(num_cpu))\n",
    "\n",
    "    for i,data in enumerate(results):\n",
    "        if i == 0:\n",
    "            output = data\n",
    "        else:\n",
    "            output += data\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = parallel(merge_text, merged_reviews['text'],  8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/opt/ml/wine/data/text_data.txt'\n",
    "from tqdm import tqdm\n",
    "with open(file_path, 'w') as file:\n",
    "    # Set the total file size for the progress bar\n",
    "    total_size = len(text_data)\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True, ncols=80, desc=\"Writing\") as pbar:\n",
    "        # Write the text data to the file in chunks\n",
    "        for chunk in text_data:\n",
    "            file.write(chunk)\n",
    "            pbar.update(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "total_lines = len(text_data)\n",
    "with tqdm(total=total_lines, ncols=80, desc=\"Processing\") as pbar:\n",
    "    for text in text_data:\n",
    "        text = text.strip()\n",
    "        if text:\n",
    "            dataset.append(text)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining\n",
    "\n",
    "config = BertConfig(    \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,    # layer num\n",
    "    num_attention_heads=8,    # transformer attention head number\n",
    "    intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "class TextDatasetForNextSentencePrediction(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        short_seq_probability=0.1,\n",
    "        nsp_probability=0.5,\n",
    "    ):\n",
    "        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        self.short_seq_probability = short_seq_probability\n",
    "        self.nsp_probability = nsp_probability\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory,\n",
    "            \"cached_nsp_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "        # sentence boundaries for the \"next sentence prediction\" task).\n",
    "        # (2) Blank lines between documents. Document boundaries are needed so\n",
    "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        #\n",
    "        # Example:\n",
    "        # I am very happy.\n",
    "        # Here is the second sentence.\n",
    "        #\n",
    "        # A new document.\n",
    "\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                # 여기서부터 본격적으로 dataset을 만듭니다.\n",
    "                self.documents = [[]]\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    while True: # 일단 문장을 읽고\n",
    "                        line = f.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        line = line.strip()\n",
    "\n",
    "                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n",
    "                        # 즉, 문단 단위로 데이터를 저장합니다.\n",
    "                        if not line and len(self.documents[-1]) != 0:\n",
    "                            self.documents.append([])\n",
    "                        tokens = tokenizer.tokenize(line)\n",
    "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        if tokens:\n",
    "                            self.documents[-1].append(tokens)\n",
    "                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n",
    "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
    "                self.examples = []\n",
    "                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n",
    "                for doc_index, document in enumerate(self.documents):\n",
    "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
    "        \"\"\"Creates examples for a single document.\"\"\"\n",
    "        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n",
    "        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n",
    "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "\n",
    "        # 여기가 재밌는 부분인데요!\n",
    "        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n",
    "        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다 :-)\n",
    "        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n",
    "        target_seq_length = max_num_tokens\n",
    "        if random.random() < self.short_seq_probability:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # 데이터 구축의 단위는 document 입니다\n",
    "        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n",
    "        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
    "                    tokens_b = []\n",
    "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
    "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "                        # 여기서 랜덤하게 선택합니다 :-)\n",
    "                        random_document = self.documents[random_document_index]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Actual next\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
    "                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
    "                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
    "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                        while True:\n",
    "                            total_length = len(tokens_a) + len(tokens_b)\n",
    "                            if total_length <= max_num_tokens:\n",
    "                                break\n",
    "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                            assert len(trunc_tokens) >= 1\n",
    "                            # We want to sometimes truncate from the front and sometimes from the\n",
    "                            # back to add more randomness and avoid biases.\n",
    "                            if random.random() < 0.5:\n",
    "                                del trunc_tokens[0]\n",
    "                            else:\n",
    "                                trunc_tokens.pop()\n",
    "\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    # add special tokens\n",
    "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                    \n",
    "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
    "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
    "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(example)\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,  \n",
    "    strip_accents=False,    \n",
    "    lowercase=True,\n",
    ")\n",
    "wp_tokenizer.train(\n",
    "    files=\"/opt/ml/wine/data/text_data.txt\",\n",
    "    vocab_size=40000,  \n",
    "    min_frequency=3,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer.save_model(\"/opt/ml/wine/code/models\", \"review_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file='/opt/ml/wine/code//text/models/review_tokenizer-vocab.txt',\n",
    "    max_len=128,\n",
    "    do_lower_case=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='/opt/ml/wine/data/text_data.txt',\n",
    "    block_size=128,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_probability=0.2,\n",
    "    nsp_probability=0.5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/opt/ml/wine/code/models',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1000,\n",
    "    per_gpu_train_batch_size=32,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/opt/ml/wine/code/models/model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('/opt/ml/wine/code/text/models/model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(df, model):\n",
    "    review_vectors = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in tqdm(range(len(df))):\n",
    "            reviews = df['text'][i].split('.')\n",
    "            id = df['wine_id'][i]\n",
    "            review_vector = []\n",
    "\n",
    "            for text in tqdm(reviews):\n",
    "                try:\n",
    "                    encoded_input = tokenizer.encode_plus(\n",
    "                        text, \n",
    "                        truncation = True,\n",
    "                        add_special_tokens=True, \n",
    "                        return_tensors='pt')\n",
    "                    model_output = model(**encoded_input)\n",
    "                    embeddings = model_output.last_hidden_state\n",
    "                    sentence_embedding = torch.mean(embeddings[0], dim=0)\n",
    "                    review_vector.append(sentence_embedding)\n",
    "\n",
    "                    del embeddings\n",
    "                    del model_output\n",
    "                    del encoded_input\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except: 1\n",
    "\n",
    "            del review_vector\n",
    "            del sentence_embedding\n",
    "            gc.collect()\n",
    "            mean_vector = torch.mean(torch.stack(review_vector), dim=0).numpy()\n",
    "            review_vectors[id] = mean_vector\n",
    "        \n",
    "    return review_vectors\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_vectors = parallel_embedding(merged_reviews, model, 8)\n",
    "review_vectors = get_embedding(merged_reviews, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
